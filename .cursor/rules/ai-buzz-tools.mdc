---
description: AI-Buzz Tools - Core principles and decision framework for all tools
alwaysApply: true
---

# AI-Buzz Tools

## Mission

**Help developers make better decisions about AI APIs.**

Every tool serves one purpose: give developers the information they need, instantly, with zero friction.

## Core Principles

### 1. Maximize User Value

Before building anything, ask: "Does this help a developer make a better decision faster?" If not, don't build it.

### 2. Simplicity Over Features

- One job, done exceptionally well
- Fast beats feature-rich
- Self-contained widgets (no external dependencies)
- Stateless APIs (no database required)

### 3. Trust Through Transparency

- Show data freshness ("Updated Jan 28")
- Link to official sources
- Clear error messages
- Honest about limitations

### 4. Zero Friction

- No signup required
- Optional email capture (non-blocking)
- Works in 2 clicks
- Shareable URLs

## Architectural Philosophy

These principles guide technical decisions. When in doubt, return to these.

### 1. Boring Technology Wins

Use well-understood, proven tools. FastAPI, vanilla JS, JSON files — not because they're exciting, but because they're predictable. Debugging time matters more than resume keywords.

### 2. YAGNI (You Aren't Gonna Need It)

Don't build for hypothetical future requirements. Build for today's users. If you need it later, add it later. Most "future-proofing" is wasted effort.

### 3. Prefer Reversible Decisions

Choose options that are easy to undo. File-based JSON over databases (easy to change). Stateless APIs (easy to redeploy). No vendor lock-in. The best architecture preserves optionality.

### 4. Design for Failure

Assume external services will fail. Mailchimp down? Log and continue. API timeout? Show graceful error. Every integration point is a failure point — handle it explicitly.

### 5. Dependencies Are Liabilities

Every dependency is code you don't control. It can break, have security issues, or stop being maintained. The current 8-dependency stack is intentional. New dependencies need strong justification.

### 6. Deletion Is a Feature

The best code is no code. Before adding, ask: can we delete something instead? Removing unused stubs is better than leaving dead code. Simplicity compounds.

### 7. Optimize for Comprehension

Code is read 10x more than it's written. Obvious beats clever. Consistent beats optimal. A junior developer should understand any file in under 5 minutes.

### 8. Ship, Then Iterate

Perfect is the enemy of deployed. Ship the minimum useful version. Real user feedback beats hypothetical requirements. Iteration is cheaper than prediction.

## Architecture Patterns

**All tools follow these patterns:**

- **FastAPI** with Pydantic models for validation
- **Self-contained widgets** (HTML+CSS+JS in one file)
- **JSON data files** with `_metadata` section
- **Mailchimp integration** for email (graceful fallback if not configured)
- **Share URLs** point to `ai-buzz.com` (never Render URLs)
- **Mobile-first** responsive design (test at 375px)

## Decision Framework

When choosing between options:

1. **Which maximizes user value?** → Choose that
2. **Which is simpler?** → Choose that
3. **Which is faster?** → Choose that
4. **Which builds trust?** → Choose that

If a feature doesn't clearly help users, don't build it.

## Code Quality

- Write code that's easy to understand
- Extract shared utilities (don't repeat yourself)
- Use type hints
- Handle errors gracefully
- Test core functionality

## Decision Priority

When principles conflict, choose in this order:

1. **Ship working code > Perfect code** — Done beats perfect. Ship MVP, iterate if it gets traction.
2. **User value > Developer convenience** — If it helps users, do it even if it's more work.
3. **Simple solution > Clever solution** — Obvious code beats clever code.
4. **Copy existing pattern > Invent new pattern** — Consistency beats novelty.
5. **Ask if unsure > Guess and build wrong thing** — 5 minutes of clarification saves hours of rework.

### When In Doubt

- **Scope unclear?** → Build the minimum viable version
- **Multiple approaches?** → Ask which to use before building
- **New pattern needed?** → Check if an existing pattern works first
- **Feature feels out of scope?** → Ask before building

## Boundaries

**Don't do these without explicit approval:**

- Don't add new pip dependencies (keep deployment simple)
- Don't create new architectural patterns when existing ones work
- Don't refactor working code unless explicitly asked
- Don't build features not in the current tools roadmap
- Don't optimize prematurely — ship first, optimize if needed
- Don't change `api/shared.py` without considering impact on all tools
- Don't use React/Vue/build tools — vanilla HTML/CSS/JS only
- Don't point share URLs to Render — always use ai-buzz.com

## Reference Implementation

**When building a new tool, copy from error_decoder.** It's the cleanest, most complete implementation.

### Files to Copy From

| Component      | Reference File                      | What to Copy                                                                                             |
| -------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **API Router** | `api/error_decoder.py`              | Pydantic models at top, helper functions in middle, endpoints at bottom, error handling pattern, logging |
| **Data File**  | `data/error_patterns.json`          | `_metadata` section structure, consistent data format                                                    |
| **Widget**     | `widgets/error_decoder_widget.html` | CSS variables, self-contained structure, loading states, error handling, mobile responsive patterns      |
| **Tests**      | `tests/test_error_decoder.py`       | Class-per-endpoint structure, fixtures usage, edge case coverage                                         |

### Code Patterns to Follow

**API Error Handling:**

```python
try:
    data = load_json_data("your_data.json")
    # ... process ...
except FileNotFoundError:
    logger.error("your_data.json not found")
    raise HTTPException(status_code=500, detail="Data not available.")
except Exception as e:
    logger.error(f"Error processing: {str(e)}", exc_info=True)
    raise HTTPException(status_code=500, detail=f"Error: {str(e)}")
```

**Widget CSS Variables (copy these exactly):**

```css
--primary-color: #2563eb;
--primary-hover: #1d4ed8;
--success-color: #10b981;
--error-color: #ef4444;
--warning-color: #f59e0b;
--text-primary: #1f2937;
--text-secondary: #6b7280;
--bg-primary: #ffffff;
--bg-secondary: #f9fafb;
--border-color: #e5e7eb;
--border-radius: 12px;
```

**Widget API Base URL (at top of JS):**

```javascript
const API_BASE = window.API_BASE_URL || "https://ai-buzz-tools.onrender.com";
```

## Testing

Run `pytest` before considering any tool complete. See `TESTING.md` for full documentation.

**Quick commands:**

```bash
pytest                    # Run all tests
pytest -v                 # Verbose output
make test                 # All tests via Make
make coverage             # With coverage report
```

**Pre-push hook:** Install with `./scripts/install-hooks.sh` - runs tests before push to main.

**CI:** GitHub Actions enforces 80% coverage on every push/PR.

## Tools

### Pricing Calculator

**Purpose:** Compare AI API costs instantly

### Status Page

**Purpose:** Know when AI APIs are down

### Error Decoder

**Purpose:** Understand and fix API errors

Each tool is independent but shares the same principles and patterns.

---

## Thinking Lenses

When making decisions, consider these perspectives to ensure well-rounded thinking:

### 1. Product Marketer

**Focus:** Conversion, user acquisition, CTAs, value proposition

**Questions to ask:**

- "What's the hook that makes someone want to use this?"
- "Why would someone share this with a colleague?"
- "What's the clear call-to-action?"
- "Does this solve a problem people are actively searching for?"
- "What's the value proposition in one sentence?"

**When to apply:** Content creation, feature prioritization, messaging decisions

### 2. SEO Specialist

**Focus:** Search intent, keywords, content structure, internal linking

**Questions to ask:**

- "What would someone Google to find this?"
- "Is the H1 targeting a real search query?"
- "Are we answering the user's search intent?"
- "What long-tail keywords can we capture?"
- "How can we improve internal linking?"

**When to apply:** Content pages, page titles, meta descriptions, URL structure

### 3. Developer User Advocate

**Focus:** Real pain points, what devs actually need, avoiding feature bloat

**Questions to ask:**

- "Would I actually use this tool?"
- "Does this solve a real problem I've had?"
- "Is this faster than what I'd do manually?"
- "What's the simplest version that still works?"
- "Am I adding features nobody asked for?"

**When to apply:** Feature design, tool functionality, UX decisions

### 4. Pragmatic Skeptic

**Focus:** Challenge assumptions, prevent over-engineering, question necessity

**Questions to ask:**

- "Do we actually need this?"
- "What's the simplest version that works?"
- "What can we delete instead of add?"
- "Is this solving a real problem or a hypothetical one?"
- "Can we ship this in 1 hour instead of 10?"

**When to apply:** All decisions, especially when scope is expanding

### Decision-Making Process

**After evaluating with all lenses, make a decision.** Don't ask questions unless:

- The request is genuinely ambiguous and could be interpreted multiple ways
- Multiple valid approaches exist with meaningful tradeoffs that require user input
- The decision would require adding new dependencies or changing shared code affecting multiple tools

**Default to action:** Use the thinking lenses to evaluate, then choose the best path forward based on:

1. Which maximizes user value?
2. Which is simpler?
3. Which is faster?
4. Which builds trust?

Then implement. Iteration beats perfection.

---

## Analytics-Driven Iteration

Measure what matters, then iterate based on data.

### What to Measure

**Tool Usage:**

- Which tools get used most? Which actions?
- Are users completing the main action (decode, calculate, check)?
- Where do users drop off?

**Content Performance:**

- Which pages drive tool usage?
- What's the conversion: page view → tool use → email signup?
- Which search queries are bringing traffic?

**Conversion Funnel:**

- Page view → tool use → email signup
- Track each step to identify bottlenecks

### How to Use Data

**Weekly Review:**

- Check GA4 dashboard weekly (manual for now)
- Look at Events → tool_used, email_signup, share_created
- Review which pages drive the most tool usage

**Decision Framework:**

- If a tool isn't being used: Is it a discovery problem (they don't know it exists) or a value problem (it doesn't solve their need)?
- If content isn't converting: Test different CTAs, page structure, or messaging
- Double down on what works, deprecate what doesn't

**Iteration Process:**

1. Identify the problem from data (low usage, high bounce, etc.)
2. Form a hypothesis (e.g., "Users don't understand the value")
3. Make a small change (e.g., improve intro copy)
4. Measure the impact (check GA4 after 1-2 weeks)
5. Iterate or move on

### When to Automate

**Current State:** Use GA4's web interface manually

**When to Add Automation:**

- Add automated pull script when you have 4+ weeks of meaningful data
- Until then, manual checks are sufficient
- Don't over-engineer analytics before you have traffic

**Future Automation:**

- Script to pull GA4 data into JSON file
- Weekly automated reports
- Dashboard for quick insights

### Analytics Events

All widgets track these events via GA4:

- `tool_used` - User completes main action (decode, calculate, check)
- `email_signup` - User subscribes to alerts
- `share_created` - User copies share URL

Each event includes `tool_name` parameter for filtering.
